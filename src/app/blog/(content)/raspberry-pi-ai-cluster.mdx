import Image from 'next/image';
import { SchemaBuilder } from '@/components/Antigravity/SchemaBuilder';
import Breadcrumbs from '@/components/Antigravity/Breadcrumbs';
import { InteractiveContainer } from '@/components/Antigravity/InteractiveContainer';
import { QuizEngine } from '@/components/Antigravity/QuizEngine';
import { ContextCollapse } from '@/components/Antigravity/ContextCollapse';

export const metadata = {
  title: "The Raspberry Pi AI Cluster: A Toaster That Thinks?",
  description: "We chained 8 Raspberry Pi 5s together to run Llama-3. It's slow. It's stupid. It's magnificent.",
  openGraph: {
    images: ['/images/blog/pi-cluster-thumb.png'],
    tags: ['Raspberry Pi', 'Homebrew', 'Llama-3', 'Cluster']
  }
};

<SchemaBuilder 
  article={{
    headline: "The Raspberry Pi AI Cluster: A Toaster That Thinks?",
    description: "We chained 8 Raspberry Pi 5s together to run Llama-3. It's slow. It's stupid. It's magnificent.",
    image: "https://gaymensfieldguide.com/images/blog/pi-cluster-thumb.png",
    datePublished: "2025-01-15",
    author: "Architect"
  }}
  breadcrumbs={[
    { name: "Home", item: "/" },
    { name: "Blog", item: "/blog" },
    { name: "Pi AI Cluster", item: "/blog/raspberry-pi-ai-cluster" }
  ]}
/>

<Breadcrumbs items={[
  { label: 'Home', href: '/' },
  { label: 'Blog', href: '/blog' },
  { label: 'The Thinking Toaster' }
]} />

{/* HEADER */}
<div className="mb-12 border-b border-dashed border-zinc-700 pb-8">
  <h1 className="text-4xl md:text-6xl font-black tracking-tighter text-transparent bg-clip-text bg-gradient-to-r from-neon-green to-banana-500 mb-4 uppercase glitch-text">
    The Thinking Toaster
  </h1>
  <div className="flex items-center gap-4 font-mono text-xs text-zinc-500">
    <span className="bg-zinc-900 px-2 py-1 rounded text-banana-500 border border-banana-500/20">PROJECT_LOG</span>
    <span>NODES: 8</span>
    <span>SPEED: 2 t/s</span>
  </div>

{/* IMAGE PROMPT:
[SUBJECT]: A stack of 8 Raspberry Pi circuit boards with glowing LEDs and messy rainbow ribbon cables
[STYLE]: Nano Banana aesthetic - dark industrial paper texture, neon yellow accents
[ELEMENTS]: "CLUSTER_ONLINE" text overlay
[MOOD]: Cyberpunk junk, DIY, messy
[FORMAT]: Wide aspect ratio
*/}
<div className="relative w-full aspect-video bg-zinc-900 border border-zinc-800 mb-12 flex items-center justify-center overflow-hidden group">
  <Image src="/images/blog/rpi-cluster-mess.png" alt="RPI_CLUSTER_MESS" fill className="object-cover" priority />
</div>
</div>

## Why? (Because We Can)

Let's get this out of the way: **This is not practical.**

For the price of 8 Raspberry Pi 5s (8GB), switches, and SD cards, you could buy a used GPU that is 50x faster.

But that's not the point. The point is learning how **Distributed Inference** works.

## The Setup: "Distributed Llama"

We used a project called [Distributed Llama](https://github.com/b4rtaz/distributed-llama). It uses tensor parallelism to split the model across devices over Ethernet.

-   **Head Node:** Pi 5 (Orchestrator)
-   **Worker Nodes:** 7x Pi 5s
-   **Network:** Gigabit Switch (The bottleneck)

### The Bottleneck is Physics

<InteractiveContainer title="LATENCY_SIMULATION" type="simulation">
  <div className="space-y-4 font-mono text-xs">
    <div className="flex items-center gap-4">
      <span className="w-20 text-right text-zinc-500">GPU VRAM</span>
      <div className="flex-1 h-2 bg-zinc-800 rounded overflow-hidden">
         <div className="h-full bg-neon-green w-full animate-pulse"></div>
      <span className="w-24 text-neon-green">1,000 GB/s</span>
    </div>
    <div className="flex items-center gap-4">
      <span className="w-20 text-right text-zinc-500">Ethernet</span>
      <div className="flex-1 h-2 bg-zinc-800 rounded overflow-hidden">
         <div className="h-full bg-banana-500 w-[1%]"></div>
      <span className="w-24 text-banana-500">0.1 GB/s</span>
    </div>
  <p className="mt-4 text-center text-zinc-400 italic">
    "Trying to run an LLM over Ethernet is like trying to drink a milkshake through a coffee stirrer."
  </p>
  </div>
  </div>
  </div>
</InteractiveContainer>

<QuizEngine 
  title="CLUSTER_REALITY_CHECK"
  type="game"
  questions={[
    {
      id: 1,
      text: "Is a Pi Cluster cheaper than a Mac Mini?",
      options: [
        { label: "Yes, way cheaper", isCorrect: false },
        { label: "No, 8 Pis + Switch = $700+", isCorrect: true, feedback: "Correct. It is actually terrible value." }
      ]
    }
  ]}
/>

## The Results

We loaded **Llama-3-8b-Instruct**.

-   **Boot Time:** 45 seconds (Loading weights over network).
-   **Inference Speed:** ~14 tokens/second.

Wait. **14 t/s?** That's... actually usable.

It turns out for small (8b) models, the Pi 5's memory bandwidth is just fast enough to be readable. It reads at the speed of human reading.


<ContextCollapse />

## Verdict

Don't build this to do work. Build this to understand how Google's TPU pods work. It is a miniature, slow, educational supercomputer.

And it looks really cool on a shelf.
